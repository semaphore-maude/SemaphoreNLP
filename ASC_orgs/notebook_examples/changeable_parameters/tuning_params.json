{
    "main_col": ["Mandate/Mission", 
                 "Main Community-Engaged Arts / Arts for Social Change Activities", 
                 "Additional Info",
                 "ArtBridging Section",
                 "Arts Focus"],

    "bc_col":   ["Mandate/Mission", "Additional Info"], 

    "world_col":["Mission"],

    "set_kwargs":{"kwargsLDA":    {"update_every": 0, 
                                   "passes": 40,
                                   "iterations": 5000,
                                   "minimum_probability": 0.1, 
                                   "per_word_topics": 1, 
                                   "minimum_phi_value": 0.02,    
                                   "alpha": "auto"},

                  "help_kwargsLDA":{"update_every": ["int", "number of docs iterated through for each update - set to 0 for batch learning\n"], 
                                    "passes": ["int", "number of passes through corpus during training\n"],
                                    "iterations": ["int", "max iterations through corpus when inferring topic distribution\n"],
                                    "minimum_probability": ["float (0,1)", "min topic probability (discarded if under)\n"],
                                    "per_word_topics": ["bool", "if True, also computes a list of topics, sorted in descending order of most likely topics for each word,\nalso computes lower bound on expected word count in topic (phi*len(feature))\n"],
                                    "minimum_phi_value": ["float (0,1)", "min per word probability (discarding if under). `per_word_topics` must be set to True\n"],    
                                    "alpha": ["float (0,1) or str", "topic prob. prior, can set each topic prior individually.\nstr choices: ['auto': Empirical Bayes, 'asymmetric': 1/(topic_number)]\n"]},

                  "kwargs_w2v":    {"min_count": 2,
                                    "size": 100,
                                    "window": 4,
                                    "iter": 100,
                                    "sg": 1,
                                    "alpha": 0.05,
                                    "min_alpha": 0.005,
                                    "sample": 1e-5,
                                    "negative": 5,                                    
                                    "hs": 0,
                                    "ns_exponent": 0.75,
                                    "cbow_mean": 0,
                                    "compute_loss": 1},

                  "help_kwargs_w2v": {"min_count": ["int", "ignore words that appear less than _"],
                                      "size": ["int", "dimensionality of word vectors"],
                                      "window": ["int", "max distance between current and predicted word in a sentence"],
                                      "iter": ["int", "number of epochs training over corpus"],
                                      "sg": ["bool", "training algorithm: skip-gram (1) or CBOW (continuous bag-of-words) (0)"],
                                      "alpha": ["float (0,1)", "initial learning rate"],
                                      "min_alpha": ["float (0,alpha)", "lr linearly drops to min_alpha in training"],
                                      "sample": ["float usually (0, 1e-5)", "higher-frequency words are randomly downsampled"],
                                      "negative": ["int usually [5,20]", "how many “noise words” are drawn. If 0, no negative sampling"],                                      
                                      "hs": ["bool", "hierarchical softmax on training. If 0 and negative is non-zero, negative sampling"],
                                      "ns_exponent": ["float usually 0.75", "exponent to shape the negative sampling dist. 1.0 samples \nexactly in proportion to the freqs, 0.0 samples all words equally; a negative value samples low-freq words more than high-freq words"],
                                      "cbow_mean": ["bool", "If 0, use sum of context word vectors. If 1, use mean, (only applies with `sg=0` (cbow))."],
                                      "compute_loss": ["bool", "if True, store loss, can be retrieved with `get_latest_training_loss()`"]},

                  "kwargs_d2v":  {"window": 4,       
                                  "min_count": 2, 
                                  "alpha": 0.05, 
                                  "dm": 0,                               
                                  "sample": 1e-5,
                                  "negative": 5,                                  
                                  "hs": 0,
                                  "ns_exponent": 0.75,                                  
                                  "vector_size": 200,  
                                  "dbow_words": 1,
                                  "dm_mean": 0,   
                                  "dm_concat": 1,
                                  "dm_tag_count": 1},

                  "help_kwargs_d2v":  {"window": ["int", "max distance between current and predicted word within a sentence"],       
                                       "min_count": ["int", "ignores all words with total frequency lower than this"], 
                                       "alpha": ["float", "initial learning rate"], 
                                       "dm": ["bool", "training algorithm. If 1: ‘distributed memory’ (PV-DM). if 0: distributed bag of \nwords (PV-DBOW) is used"],                               
                                       "sample": ["float", "usually (0,1e-5), threshold for higher-frequency words random downsampling"],
                                       "negative": ["int", "usually [5,20], how many “noise words” are drawn. If 0, no negative sampling"],                                       
                                       "hs": ["bool", "if 1, hierarchical softmax for model training. If 0 and `negative` is non-zero, \nnegative sampling is used"],
                                       "ns_exponent": ["float", "usually 0.75, exponent to shape the negative sampling dist. 1.0 samples \nexactly in proportion to the freqs, 0.0 samples all words equally; a negative value samples low-freq words more than high-freq words"],                                       
                                       "vector_size": ["int", "dimensionality of feature vectors"],  
                                       "dbow_words": ["bool", "If 1 trains word-vectors (in skip-gram fashion) simultaneous with DBOW \ndoc-vector training; If 0, only trains doc-vectors (faster)"],
                                       "dm_mean": ["bool", "if 0, use sum of context word vectors. If 1, use mean (only when dm_concat=0)"],   
                                       "dm_concat": ["bool", "If 1, use concatenation of context vectors rather than sum/average; Note \nconcatenation results in a much-larger model, as the input is no longer the size of one (sampled or arithmetically combined) word vector, but the size of the tag(s) and all words in the context strung together"], 
                                       "dm_tag_count": ["bool", "expected number of document tags per document, when using dm_concat mode."]}
                  },



    "added_stopwords": ["art", "arts", "artists", "artist", "scarborough", "toronto", "vancouver", 
                        "yukon", "john", "community", "hamilton", "regent", "essex", "ontario", 
                        "canada", "bathurst", "sfu","simon", "fraser", "zoukak", "yac", "wpts", 
                        "wp", "formaat","metchosin", "nfp", "melbourne", "iom", "bmw", "terry",
                        "eastside-based", "ubw", "bush","pennsylvania", "flores", "jose", "las", 
                        "yole", "westside","vidya", "lehigh", "tnt", "tmc", "parminou", "wychwood",
                        "ste-emilie", "richard", "czkd", "lin", "maya", "commuthe","trillium", 
                        "alexander", "duniya", "valle", "argine","lampung", "karang", "tanjung", 
                        "pratama", "iswadi", "garasi", "nscad", "msvu","dalhousie", "renee", "ds", 
                        "bennett", "launceston", "wa", "andrew", "hqs","nsw", "george", "talanta", 
                        "saint", "x", "appalachian", "appalachia", "east-side", "mb", "aka","atsa", 
                        "roy", "annie", "allard", "pierre", "prishtina","wildwood", "sd", "houston", 
                        "ngueniene", "alvares","cecile-guidote", "philadelphia-area", "selpak", 
                        "ponleu", "situ","cochabamba", "ppp", "peng", "calderdale", "hebden", 
                        "sony", "wawesh", "nauta","nynke", "penya", "killaloe", "solphonic", 
                        "solfonica", "orquesta", "kaos", "assosa","olt", "oboro", "abdulleh", 
                        "kinsi", "ing", "ative", "cre", "er","deliv", "zation", "ni", "orga", 
                        "heggodu","rhode", "providence", "girouard", "notre-dame-de-grce", "mu", 
                        "fronteras", "movimiento", "mikwchiyam", "martadero", "nyt", "mtyp",
                        "mcg", "madaboutart", "m-lisada", "ouissi", "sofiane", "selma","lapd", 
                        "tetes", "centre-sud", "montreal", "tarumba", "esquina", "torres",
                        "mizik", "konbit", "mcmurchy", "geoffrey", "kcyp", "kaippg", "nah", "vince", 
                        "amsterdam","lovink", "geert", "forma", "imove", "ikapa", "idensitat", 
                        "dan", "hua","gudran", "nineteen", "francisco", "francisco-based", "svitac",
                        "exeko", "kampot","enmedio", "sistema", "cuerpo", "colegio", "sixth", 
                        "dodolab","ph", "daniel","mckinney", "adam", "huntly", "bothies", 
                        "anishinaabeg", "de-ba-jeh-mu-jig","darb", "witha","danza", "dieu", "bon",
                        "johns", "emmett", "ethekwini", "australian", "cacd","peop", "barrett", 
                        "luce", "emily", "myrdahl", "muller", "tiffany","doolittle", "mackay", 
                        "bruce", "investigator", "principal", "mills", "josephine","cal-xl", "dutch", 
                        "lab-xl", "kentish", "quatre", "ciblee", "entrer", "volador", "novgorod", 
                        "nizhny", "moscow", "delat", "chto","chol", "candoco", "ludica", "caja", 
                        "alland", "ccc", "candg", "scottish", "cachunga", "cachan","cachin", 
                        "king", "chharas", "brock", "canadas", "belfast", "belarus", "yayem", 
                        "ashtar", "orleans", "pelham", "perth", "davenport","awi", "aps", "westminster", 
                        "braid", "dis", "lewa", "ard", "artellewa", "ticah", "elizabeth", "arepp", 
                        "esteras", "aida", "fortes", "elena", "cruz", "pablo", "luna", "diego", 
                        "bernal", "garcia", "gael", "alrowwad", "ajoka", "wikiafrica", "badilisha", 
                        "x-change","laumeier", "louis", "detroit", "michigan", "iraq", "carter", 
                        "garner", "angeles-based", "fm","montessori", "pittsburgh", "nebraska", 
                        "omaha", "arkansas", "diane", "missouri-kansas", "afta","dick","shannon", 
                        "lynn", "levesque", "olie", "polie", "rolie", "dc", "hanks", "suvero", "di",
                        "kurtz", "quinn", "marc", "portland", "louisiana", "mandeville", "julia", 
                        "martinez", "valerie","sol", "hidalgo", "opry", "uaf", "uafs", "karl", 
                        "kuehner", "leon", "wyoming", "tx","wi", "sauk", "darwin", "charles", 
                        "etps", "otc", "foote", "stevens", "chicago", "alaskas", "lt","lynch", 
                        "bob", "morrison", "mullick", "nirvan", "jackson", "yvette", "fresno", 
                        "clm", "gianna","aragon", "carolina", "weiner", "lawerence", "bostonians", 
                        "paris", "mta", "sixteen", "fitzgerald","massachusetts","one", "americans", 
                        "us", "two", "three", "st", "four", "calgary", "east", "inc", "bc","york", 
                        "manitoba", "r_e", "alberta", "montreal", "nashville", "etp", "iowa", "five", 
                        "momo", "jackson", "oklahoma", "el", "amp", "six", "oct", "america", "ten", 
                        "american","denver", "cac", "greenway", "david", "australia", "west", "e", 
                        "shreveport","july","august", "september", "nova", "via", "gta", "james", 
                        "june", "amy", "sistema","cuyahoga", "kaiser_permanente", "tv", "halifax", 
                        "scotia", "october", "regina","sunday", "cacv", "seven", "edmonton", 
                        "winnipeg", "icmi", "january", "oscs", "sudbury", "afta","san", "william", 
                        "haiti", "ndg", "bonnie", "canada","canadians","sault", "ste","marie", 
                        "monday", "alaska", "yqmp", "ftlol", "fred", "dawson", "augusto","ottawa", 
                        "eastside", "isaac", "murdoch", "b", "thursday", "november", "del", "safer", 
                        "uas","djing", "veith", "ca", "oregon", "albuquerque", "texas", "u", "arizona", 
                        "los", "quebec", "uk","lisa", "sid", "ccac", "cpt", "april", "tlingit", 
                        "damelahamid", "eight", "h'art","saturday", "harcourt", "ave", "boal", 
                        "-", "mabelle", "mabelleart", "nina", "haggerty", "tps","obsidian", "onaman", 
                        "erin", "dehcho", "pac", "vancouver-based", "toronto", "saskatoon", "scyap",
                        "usay", "nampc", "dtour", "harwood", "sanchez", "srac", "nccil", "m-aaa", 
                        "kansas", "scott", "g","artstarts", "bak", "les", "sydney", "dala", "darpana", 
                        "jaf", "klein", "colmena", "icasc", "iyia","saar", "r",
                        "also", "etc", "nipiss", "lot",  "le","artheart", "loopz","twelve", "eleven", 
                        "entr'elles", "we're", "mabellearts", "that","bridgewater","we've","k",
                        "nine", "prh", "rexdale", "dtes", "penny", "calgary", "stephen","john", 
                        "victor", "what", "matt", "freire", "london", "i've","i'd", "vivo",
                        "judith", "marcus", "there", "l'organisme", "l'observatoire", "hhh",
                        "saskatchewan", "palestinian", "paulo", "saludart", "catherine",
                        "cambodian", "northwestern", "miscellaneous", "pakistan", "michael",
                        "fourteen", "egyptian", "quendra", "african", "british_colombia", "hrm",
                        "artreach", "mexico", "wormfarm institute", "artsexpress", 
                        "formaat", "thus", "jamaica"],

    "client_groups":{"indigenous":   ["aboriginal"],
                     "incarcerated": ["prison"],
                     "homeless":     ["poverty"],
                     "lgbt":         ["queer"],
                     "women":        ["female", "man"],
                     "youth":        ["youth", "senior"]}
    } 
     


            
